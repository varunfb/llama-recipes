{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt Formatting Sample\n",
        "\n",
        "This notbook is intended to showcase how to correctly format a Llama 3 prompt.\n",
        "\n",
        "Open in <a href=\"https://colab.research.google.com/github/varunfb/llama-recipes/blob/main/tools/prompt_generator/prompt_sample.ipynb\"><img data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" src=\"https://camo.githubusercontent.com/f5e0d0538a9c2972b5d413e0ace04cecd8efd828d133133933dfffec282a4e1b/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import (\n",
        "    List,\n",
        "    Literal,\n",
        "    Sequence,\n",
        "    TypedDict,\n",
        ")\n",
        "\n",
        "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
        "\n",
        "class Message(TypedDict):\n",
        "    role: Role\n",
        "    content: str\n",
        "\n",
        "\n",
        "Dialog = Sequence[Message]\n",
        "\n",
        "class ChatFormat:\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_header( message: Message) -> List[str]:\n",
        "        prompts = []\n",
        "        prompts.append(\"<|start_header_id|>\")\n",
        "        prompts.extend(message[\"role\"])\n",
        "        prompts.append(\"<|end_header_id|>\")\n",
        "        prompts.extend(\"\\n\\n\")\n",
        "        return prompts\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_message(message: Message) -> List[str]:\n",
        "        prompts = ChatFormat.encode_header(message)\n",
        "        prompts.extend(message[\"content\"].strip())\n",
        "        prompts.append(\"<|eot_id|>\")\n",
        "        return prompts\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_dialog_prompt(dialog: Dialog) -> str:\n",
        "        prompts = [\"<|begin_of_text|>\"]\n",
        "        for message in dialog:\n",
        "            prompts.extend(ChatFormat.encode_message(message))\n",
        "        # Add the start of an assistant message for the model to complete.\n",
        "        prompts.extend(ChatFormat.encode_header({\"role\": \"assistant\", \"content\": \"\"}))\n",
        "        return \"\".join(prompts)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def format_python_prompt_output(system_prompt, user_prompt_1, assistant_response, user_prompt_2) -> str:\n",
        "        template = f\"\"\"from typing import (\n",
        "    List,\n",
        "    Literal,\n",
        "    Sequence,\n",
        "    TypedDict,\n",
        ")\n",
        "\n",
        "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
        "\n",
        "class Message(TypedDict):\n",
        "    role: Role\n",
        "    content: str\n",
        "\n",
        "\n",
        "Dialog = Sequence[Message]\n",
        "\n",
        "class ChatFormat:\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_header( message: Message) -> List[str]:\n",
        "        prompts = []\n",
        "        prompts.append(\"<|start_header_id|>\")\n",
        "        prompts.extend(message[\"role\"])\n",
        "        prompts.append(\"<|end_header_id|>\")\n",
        "        prompts.extend(\"\\\\n\\\\n\")\n",
        "        return prompts\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_message(message: Message) -> List[str]:\n",
        "        prompts = ChatFormat.encode_header(message)\n",
        "        prompts.extend(message[\"content\"].strip())\n",
        "        prompts.append(\"<|eot_id|>\")\n",
        "        return prompts\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_dialog_prompt(dialog: Dialog) -> str:\n",
        "        prompts = [\"<|begin_of_text|>\"]\n",
        "        for message in dialog:\n",
        "            prompts.extend(ChatFormat.encode_message(message))\n",
        "        # Add the start of an assistant message for the model to complete.\n",
        "        prompts.extend(ChatFormat.encode_header({{\"role\": \"assistant\", \"content\": \"\"}}))\n",
        "        return \"\".join(prompts)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dialog: Dialog = []\"\"\"\n",
        "        if system_prompt:\n",
        "            system_section = f\"\"\"\n",
        "    dialog.append({{   \"role\": \"system\", \"content\": \"{system_prompt}\", }})\"\"\"\n",
        "            template += system_section\n",
        "        user_section = f\"\"\"\n",
        "    dialog.append({{   \"role\": \"user\", \"content\": \"{user_prompt_1}\", }})\"\"\"\n",
        "        template += user_section\n",
        "        if assistant_response:\n",
        "            assistant_section = f\"\"\"\n",
        "    dialog.append({{   \"role\": \"assistant\", \"content\": \"{assistant_response}\", }})\"\"\"\n",
        "            template += assistant_section\n",
        "        if user_prompt_2:\n",
        "            second_user_prompt_section = f\"\"\"\n",
        "    dialog.append({{   \"role\": \"user\", \"content\": \"{user_prompt_2}\", }})\"\"\"\n",
        "            template += second_user_prompt_section\n",
        "\n",
        "        template += f\"\"\"\n",
        "    print(ChatFormat.encode_dialog_prompt(dialog))\"\"\"\n",
        "\n",
        "        return template\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:    [Errno 48] error while attempting to bind on address ('0.0.0.0', 7862): address already in use\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://0.0.0.0:7865\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://localhost:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "SINGLE_TURN = \"Single Turn\"\n",
        "MULTI_TURN = \"Multi Turn\"\n",
        "\n",
        "def prompt_template_dropdown_listener(value):\n",
        "    # This function will be called whenever the value of the dropdown changes\n",
        "    if value == SINGLE_TURN:\n",
        "        return {\n",
        "            assistant_response: gr.Textbox(elem_id=\"assistant_response\", visible = False),\n",
        "            user_prompt_2: gr.Textbox(elem_id=\"user_prompt_2\", visible = False)\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            assistant_response: gr.Textbox(elem_id=\"assistant_response\", visible = True),\n",
        "            user_prompt_2: gr.Textbox(elem_id=\"user_prompt_2\", visible = True)\n",
        "        }\n",
        "\n",
        "def format_prompt_template_listener(system_prompt, user_prompt_1, assistant_response, user_prompt_2, prompt_template):\n",
        "    if not user_prompt_1:\n",
        "        raise gr.Error(\"User prompt is mandatory.\")\n",
        "\n",
        "    if not user_prompt_2 and assistant_response:\n",
        "        raise gr.Error(\"When the assistant message is set, the second user prompt is mandatory.\")\n",
        "\n",
        "    if prompt_template == MULTI_TURN and ((user_prompt_2 and not assistant_response) or (not user_prompt_2 and not assistant_response)):\n",
        "        raise gr.Error(\"When generating a multi turn prompt, the assistant message is mandatory.\")\n",
        "\n",
        "    dialog: Dialog = []\n",
        "    if system_prompt: dialog.append({   \"role\": \"system\", \"content\": system_prompt, })\n",
        "    dialog.append({   \"role\": \"user\", \"content\": user_prompt_1, })\n",
        "    if assistant_response: dialog.append({   \"role\": \"assistant\", \"content\": assistant_response, })\n",
        "    if user_prompt_2: dialog.append({   \"role\": \"user\", \"content\": user_prompt_2, })\n",
        "\n",
        "    return {\n",
        "        prompt_output: ChatFormat.encode_dialog_prompt(dialog),\n",
        "        python_output: ChatFormat.format_python_prompt_output(system_prompt, user_prompt_1, assistant_response, user_prompt_2),\n",
        "    }\n",
        "\n",
        "css = \"\"\"\n",
        "h1 {\n",
        "    text-align: center;\n",
        "    display:block;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=css) as demo:\n",
        "    gr.Markdown(\"#  Llama 3 Prompt Formatter\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1, min_width=200):\n",
        "            gr.Markdown(\"## Configurations\")\n",
        "            prompt_template = gr.Dropdown([SINGLE_TURN, MULTI_TURN], label=\"Prompt Template\", filterable=False, value=SINGLE_TURN)\n",
        "\n",
        "            gr.Markdown(\"## Input Prompts\")\n",
        "            system_prompt = gr.Textbox(label=\"System prompt\", lines=2, placeholder=\"Optional System prompt for the model\")\n",
        "            user_prompt_1 = gr.Textbox(elem_id=\"user_prompt_1\", label=\"User prompt *\", lines=2, placeholder=\"User prompt required\")\n",
        "            assistant_response = gr.Textbox(label=\"Assistant response\", lines=2, visible=False, elem_id=\"assistant_response\", placeholder=\"Assistant prompt required on Multi Turn\")\n",
        "            user_prompt_2 = gr.Textbox(label=\"User prompt *\", lines=2, visible=False, elem_id=\"user_prompt_2\", placeholder=\"User prompt required on Multi turn\")\n",
        "\n",
        "            prompt_template.input(prompt_template_dropdown_listener, prompt_template, [assistant_response, user_prompt_2])\n",
        "\n",
        "            submit = gr.Button(\"Submit\")\n",
        "\n",
        "        with gr.Column(scale=3, min_width=600):\n",
        "            gr.Markdown(\"## Output\")\n",
        "            with gr.Tab(\"Preview\"):\n",
        "                prompt_output = gr.Code(show_label=True, interactive=False, min_width=600, lines=30)\n",
        "\n",
        "            with gr.Tab(\"Code\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Tab(\"Python\"):\n",
        "                        python_output = gr.Code(label=\"Python Code\", interactive=False, min_width=600, lines=30, language=\"python\")\n",
        "                    \n",
        "\n",
        "    inputs = [system_prompt, user_prompt_1, assistant_response, user_prompt_2, prompt_template]\n",
        "    outputs = [prompt_output, python_output]\n",
        "    submit.click(format_prompt_template_listener, inputs, outputs)\n",
        "\n",
        "demo.queue().launch(server_name=\"0.0.0.0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7865\n"
          ]
        }
      ],
      "source": [
        "demo.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "be550690-21f5-4075-9163-4b33aea27a8d",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "prompt_generator",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
